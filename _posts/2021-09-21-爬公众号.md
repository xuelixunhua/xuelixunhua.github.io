---
title: 【编程】爬公众号
categories:
 - 输出
 - 编程
tags:
 - 编程
 - 爬虫/自动化
---

主要是想爬渤海小吏的文章，所以就写了段代码来爬，本文爬取对象就是公众号的内容。

本来公众号有一定的反爬机制，不是随便找个网址随便爬的内容，但是我发现今年还是去年新上的功能“话题标签”给的接口很方便，接下来就来爬吧。

## 查看爬取对象

随便打开一篇文章，发现它有红框这个话题收录，这个就是忘了去年还是今年新上的一个功能，我们点开它，右键点检查进入控制台（要把网页复制到浏览器中）。

![1](https://raw.githubusercontent.com/xuelixunhua/xuelixunhua.github.io/main/assets\images\articles\programing\爬公众号\1.png)

点network（可能不同的浏览器名称不太一样），红框那的Fetch/XHR本来是空白的，点一下界面上的倒叙，可以看到出现的新内容。

![2](https://raw.githubusercontent.com/xuelixunhua/xuelixunhua.github.io/main/assets\images\articles\programing\爬公众号\2.png)

Name那里点进去出现右侧这个框，Headers这里有个request URl是我们获得的网址，也是我们主要爬的内容。

![3](https://raw.githubusercontent.com/xuelixunhua/xuelixunhua.github.io/main/assets\images\articles\programing\爬公众号\3.png)

点preview可以查看这个访问这个网址返回的内容，跟response的内容一样，是一段很长的代码，但preview这里帮我们解析成更易观察的内容，可以通过点击找到我们想要的内容，我们需要的内容是title和url那一栏，也就是具体的网址内容。

![4](https://raw.githubusercontent.com/xuelixunhua/xuelixunhua.github.io/main/assets\images\articles\programing\爬公众号\4.png)

## 爬取思路

1. 爬取我们上文找到的网址，得到每篇文章的标题和具体网址
2. 循环访问每篇文章的网址，转成pdf储存

有个较平时爬虫不同的地方是把html转成pdf储存，通过问度娘，被告知可以下个第三方库完成，那么来做吧。

## 具体过程

下载第三方库wkhtmltopdf，网址：https://wkhtmltopdf.org/downloads.html。

安装模块：`pip install pdfkit`

导入需要的模块：

```python
import pdfkit
from urllib.request import urlopen, Request
import json
import pandas as pd
```

爬取一开始找到的网址并解析成能看懂的样子，然后看网址具体部分有个`count`本来=10，我改成100了，可以获取全部的文章，否则只能获取前十篇文章，另外有个部分是`is_reverse`是=1的，代表我们刚刚做的倒序，可以改成0变成正序（不改也不影响结果）。

```python
url = 'https://mp.weixin.qq.com/mp/appmsgalbum?action=getalbum&__biz=MzUyMzUyNzM4Ng==&album_id=1339904567118741505&count=100&is_reverse=0&uin=&key=&pass_ticket=&wxtoken=&devicetype=&clientversion=&__biz=MzUyMzUyNzM4Ng%3D%3D&appmsg_token=&x5=0&f=json'
req = Request(url, headers=headers)  # headers没放上来哈
content = urlopen(req, timeout=10).read()
data = pd.DataFrame(json.loads(content))['getalbum_resp']['article_list']  # 后面两个相当于一级目录、二级目录，具体为什么这样写可以看preview的内容
data = pd.DataFrame(data)[['title', 'url']]
data
```

得到这样的结果：

![5](https://raw.githubusercontent.com/xuelixunhua/xuelixunhua.github.io/main/assets\images\articles\programing\爬公众号\5.png)

然后咱们配置那个第三方库并爬取就行了：

```python
confg = pdfkit.configuration(wkhtmltopdf=r'C:\xxxx\wkhtmltopdf.exe')  # 地址是自己电脑安装的位置
pdfkit.from_url(data['url'][1], data['title'][1],configuration=confg)  
```

但是会发现爬下来的内容没有图片，即便给个delay时间也不行，遂问度娘，度娘又扔过来一个第三方库，那么`pip install wechatsogou`并import。

```python
confg = pdfkit.configuration(wkhtmltopdf=r'C:\xxxx\wkhtmltopdf.exe')  # 地址是自己电脑安装的位置
ws_api = wechatsogou.WechatSogouAPI(captcha_break_time=3)  # 刚刚的第三方库

for title, urls in data.to_dict('split')['data']:
    content_info = ws_api.get_article_content(urls)
    html_code = content_info['content_html']

    # 处理后的html
    html = f'''
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>{title}</title>
        </head>
        <body>
        <h2 style="text-align: center;font-weight: 400;">{title}</h2>
        {content_info['content_html']}
        </body>
        </html>
        '''
        
    pdfkit.from_string(html, '%s.pdf' % title, configuration=confg)
```

ok搞定了：

![6](https://raw.githubusercontent.com/xuelixunhua/xuelixunhua.github.io/main/assets\images\articles\programing\爬公众号\6.png)